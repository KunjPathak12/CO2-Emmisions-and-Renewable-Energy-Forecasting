{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# configs\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('global-data-on-sustainable-energy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continuous features report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_continuous_features_report(data_df):\n",
    "\n",
    "    \"\"\"Build tabular report for continuous features\"\"\"\n",
    "\n",
    "    stats = {\n",
    "        \"Count\": len,\n",
    "        \"Miss %\": lambda df: df.isna().sum() / len(df) * 100,\n",
    "        \"Card.\": lambda df: df.nunique(),\n",
    "        \"Min\": lambda df: df.min(),\n",
    "        \"1st Qrt.\": lambda df: df.quantile(0.25),\n",
    "        \"Mean\": lambda df: df.mean(),\n",
    "        \"Median\": lambda df: df.median(),\n",
    "        \"3rd Qrt\": lambda df: df.quantile(0.75),\n",
    "        \"Max\": lambda df: df.max(),\n",
    "        \"Std. Dev.\": lambda df: df.std(),\n",
    "    }\n",
    "\n",
    "    contin_feat_names = data_df.select_dtypes(\"number\").columns\n",
    "    continuous_data_df = data_df[contin_feat_names]\n",
    "\n",
    "    report_df = pd.DataFrame(index=contin_feat_names, columns=stats.keys())\n",
    "\n",
    "    for stat_name, fn in stats.items():\n",
    "        # NOTE: ignore warnings for empty features\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            report_df[stat_name] = fn(continuous_data_df)\n",
    "\n",
    "    return report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build continuous features report\n",
    "con_report_df = build_continuous_features_report(data)\n",
    "con_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"categorical features are :\")\n",
    "data.select_dtypes(exclude=\"number\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical features Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_categorical_features_report(data_df):\n",
    "\n",
    "    \"\"\"Build tabular report for categorical features\"\"\"\n",
    "\n",
    "    def _mode(df):\n",
    "        return df.apply(lambda ft: \",\".join(ft.mode().to_list()))\n",
    "\n",
    "    def _mode_freq(df):\n",
    "        return df.apply(lambda ft: ft.value_counts()[ft.mode()].sum())\n",
    "\n",
    "    def _second_mode(df):\n",
    "        return df.apply(lambda ft: \",\".join(ft[~ft.isin(ft.mode())].mode().to_list()))\n",
    "\n",
    "    def _second_mode_freq(df):\n",
    "        return df.apply(\n",
    "            lambda ft: ft[~ft.isin(ft.mode())]\n",
    "            .value_counts()[ft[~ft.isin(ft.mode())].mode()]\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "    stats = {\n",
    "        \"Count\": len,\n",
    "        \"Miss %\": lambda df: df.isna().sum() / len(df) * 100,\n",
    "        \"Card.\": lambda df: df.nunique(),\n",
    "        \"Mode\": _mode,\n",
    "        \"Mode Freq\": _mode_freq,\n",
    "        \"Mode %\": lambda df: _mode_freq(df) / len(df) * 100,\n",
    "        \"2nd Mode\": _second_mode,\n",
    "        \"2nd Mode Freq\": _second_mode_freq,\n",
    "        \"2nd Mode %\": lambda df: _second_mode_freq(df) / len(df) * 100,\n",
    "    }\n",
    "\n",
    "    cat_feat_names = data_df.select_dtypes(exclude=\"number\").columns\n",
    "    cat_data_df = data_df[cat_feat_names]\n",
    "\n",
    "    report_df = pd.DataFrame(index=cat_feat_names, columns=stats.keys())\n",
    "\n",
    "    for stat_name, fn in stats.items():\n",
    "        # NOTE: ignore warnings for empty features\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            report_df[stat_name] = fn(cat_data_df)\n",
    "\n",
    "    return report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build categorical feature report\n",
    "cat_report_df = build_categorical_features_report(data)\n",
    "cat_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "num_duplicates = data.duplicated().sum()\n",
    "print(\"Number of Duplicate Rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(col):\n",
    "    \"\"\"Detect outliers in a dataframe column\n",
    "\n",
    "    Args:\n",
    "        col (pd.Series): A dataframe column\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Outliers\n",
    "    \"\"\"\n",
    "    Q1 = np.percentile(col, 25)\n",
    "    Q3 = np.percentile(col, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_step = IQR * 1.5\n",
    "\n",
    "    return col[(col < Q1 - outlier_step) | (col > Q3 + outlier_step)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_df = (\n",
    "    data.select_dtypes(\"number\")\n",
    "    .apply(lambda col: detect_outliers(col).size)\n",
    "    .rename(\"Num outliers\")\n",
    "    .to_frame()\n",
    ").query(\"`Num outliers` > 0\")\n",
    "\n",
    "outliers_df[\"Percent outliers\"] = outliers_df[\"Num outliers\"] / len(data) * 100\n",
    "outliers_df.sort_values(by=\"Percent outliers\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the types to the desired ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_functuation(density):\n",
    "    return density.replace(',','')\n",
    "data['Entity']=data['Entity'].astype(str)\n",
    "data['Density\\\\n(P/Km2)']=data['Density\\\\n(P/Km2)'].astype(str)\n",
    "data['Density\\\\n(P/Km2)']=data['Density\\\\n(P/Km2)'].apply(remove_functuation)\n",
    "data['Density\\\\n(P/Km2)']=data['Density\\\\n(P/Km2)'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values in continuous features\n",
    "missing_con_cols = con_report_df.query(\"`Miss %` > 0.0\").index\n",
    "#print(missing_con_cols)\n",
    "feature_with_null=[column for column in missing_con_cols if column not in ['Year','Primary energy consumption per capita (kWh/person)']]\n",
    "#print(feature_with_null)\n",
    "# Replacing values with mean\n",
    "data.fillna(data[feature_with_null].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values in categorical features\n",
    "cat_missing = cat_report_df.query(\"`Miss %` > 0\").index\n",
    "\n",
    "\n",
    "cat_fillna_vals = data[cat_missing].mode().squeeze()\n",
    "data = data.fillna(cat_fillna_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kw1fQX8Enllo",
    "outputId": "832a1115-f49a-4a75-9aa8-e4df928fee20"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv('global-data-on-sustainable-energy.csv')\n",
    "missing_values = data.isnull().sum()\n",
    "length=len(data)\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUcskvIcbsFz",
    "outputId": "c2c7ac20-7980-4a8d-ed39-9a8ebab1911e"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns\n",
    "#https://www.geeksforgeeks.org/remove-multiple-elements-from-a-list-in-python/\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n",
    "#https://medium.com/@bhanupsingh484/handling-missing-data-with-knn-imputer-927d49b09015\n",
    "#https://pandas.pydata.org/docs/user_guide/merging.html\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "columns=data.columns.tolist()\n",
    "feature_with_null=[column for column in columns if column not in ['Entity','Year','Primary energy consumption per capita (kWh/person)']]\n",
    "data_to_impute = data[feature_with_null]\n",
    "data_remaining = data[['Entity', 'Year', 'Primary energy consumption per capita (kWh/person)']]\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data_to_impute), columns=feature_with_null)\n",
    "data_combined = pd.concat([data_imputed, data_remaining], axis=1)\n",
    "data=data_combined\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "wZYAZnh1bt4N",
    "outputId": "6fd5d3d4-3aa7-4088-9e46-3fc034b60068"
   },
   "outputs": [],
   "source": [
    "df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtE36k7kbvTU",
    "outputId": "56e3e68d-9350-4d26-ba23-e1a32d71b500"
   },
   "outputs": [],
   "source": [
    "def rmOutliers(df, columns):\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# List of columns to check for outliers\n",
    "columns_to_check = [\n",
    "    'Access to electricity (% of population)',\n",
    "    'Access to clean fuels for cooking',\n",
    "    'Renewable-electricity-generating-capacity-per-capita',\n",
    "    'Financial flows to developing countries (US $)',\n",
    "    'Renewable energy share in the total final energy consumption (%)',\n",
    "    'Electricity from fossil fuels (TWh)',\n",
    "    'Electricity from nuclear (TWh)',\n",
    "    'Electricity from renewables (TWh)',\n",
    "    'Primary energy consumption per capita (kWh/person)',\n",
    "    'Energy intensity level of primary energy (MJ/$2017 PPP GDP)',\n",
    "    'Value_co2_emissions_kt_by_country',\n",
    "    'Renewables (% equivalent primary energy)',\n",
    "    'gdp_growth',\n",
    "    'gdp_per_capita',\n",
    "]\n",
    "print (columns_to_check)\n",
    "\n",
    "# Apply the function to each group of entities\n",
    "df_cleaned = df.groupby('Entity').apply(lambda x: rmOutliers(x, columns_to_check)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L16_C06rbxZB",
    "outputId": "c809f869-1b8a-442a-b761-2265083a1bb9"
   },
   "outputs": [],
   "source": [
    "print(f\"Total Outliers Removed considering for each country are:{df.count()-df_cleaned.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lqQi48EfrQd"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVP_Fpqff2su"
   },
   "outputs": [],
   "source": [
    "scaler= StandardScaler()\n",
    "columns=data.select_dtypes(include=['float','integer']).columns\n",
    "data[columns]=scaler.fit_transform(data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mUclJOXg27P"
   },
   "outputs": [],
   "source": [
    "label_encoder=LabelEncoder()\n",
    "data['Entity']=label_encoder.fit_transform(data['Entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVhE1fw8hKtn"
   },
   "outputs": [],
   "source": [
    "target_variables = ['Value_co2_emissions_kt_by_country', 'Renewable energy share in the total final energy consumption (%)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BPnb4hUvjGK"
   },
   "outputs": [],
   "source": [
    "# importing all the necessary models that are required to perform the regression\n",
    "#https://stackoverflow.com/questions/59489830/select-best-parameters-for-regression-model-using-gridsearch\n",
    "#https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained\n",
    "#https://www.analyticsvidhya.com/blog/2022/11/hyperparameter-tuning-using-randomized-search/\n",
    "#https://dev.to/newbie_coder/decision-tree-regression-a-comprehensive-guide-with-python-code-examples-and-hyperparameter-tuning-1f0f\n",
    "#https://stats.stackexchange.com/questions/269053/how-to-select-hyperparameters-for-svm-regression-after-grid-search\n",
    "#https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint,uniform\n",
    "\n",
    "models={\n",
    "    'Linear Regression':LinearRegression(),\n",
    "    'Decision Tree Regression': DecisionTreeRegressor(),\n",
    "    'Support Vector Regressor':SVR(),\n",
    "    'Random Forest Regressor':RandomForestRegressor()\n",
    "}\n",
    "param_grids = {\n",
    "    'Linear Regression': {\n",
    "              \"fit_intercept\": [True, False],\n",
    "             },\n",
    "    'Decision Tree Regression': {\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 4)\n",
    "    },\n",
    "    'Support Vector Regressor': {\n",
    "        'kernel': ('linear', 'rbf','poly'), \n",
    "        'C':uniform(1.5, 10),\n",
    "        'gamma': uniform(1e-7, 1e-4),\n",
    "        'epsilon':uniform(0.1,0.5)\n",
    "    },\n",
    "    'Random Forest Regressor': {\n",
    "         'n_estimators':  randint(25, 150), \n",
    "         'max_features': ['sqrt', 'log2', None], \n",
    "         'max_depth':  randint(3, 9), \n",
    "         'max_leaf_nodes': randint(3, 9),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "7OzVgCRgKOHg",
    "outputId": "69377960-b8b5-4b8f-d907-de6a168fae9d"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M23lRhvAuxhE",
    "outputId": "230104b2-8b37-4af9-ee2a-3d6b15b135cf"
   },
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model_with_base_parameters(models_dict):\n",
    "  for i in range(len(target_variables)):\n",
    "    print(f\"Below are the metrics for the target variable: {target_variables[i]}\")\n",
    "    for name, model in models_dict.items():\n",
    "      if name == 'Support Vector Regressor':\n",
    "            # Skipping RFE for SVR as it does not have the feature_importances_ or coef_ attribute\n",
    "            correlation_matrix =  data.select_dtypes(\"number\").corr()\n",
    "            target_features = correlation_matrix[target_variables[i]].drop(target_variables[i])\n",
    "            target_features = target_features[abs(target_features)>0.3].index.tolist()\n",
    "            X = data.drop(columns=target_features)\n",
    "            y = data[target_variables[i]]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "            model.fit(X_train, y_train)\n",
    "            #y_pred = model.predict(X_test)\n",
    "\n",
    "      else:\n",
    "            # Applying RFE for other models\n",
    "            rfe = RFE(estimator=model, n_features_to_select=10)\n",
    "            X = data.drop(columns=target_variables)\n",
    "            y = data[target_variables[i]]\n",
    "            fit = rfe.fit(X, y)\n",
    "            feature_ranking = pd.DataFrame({'Feature': X.columns, 'Ranking': fit.ranking_}).sort_values(by='Ranking')\n",
    "            final_features = feature_ranking[feature_ranking['Ranking'] == 1]['Feature'].to_list()\n",
    "            if 'Entity' not in final_features:\n",
    "                final_features.append('Entity')\n",
    "            X = X[final_features]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "            model.fit(X_train, y_train)\n",
    "            #y_pred = model.predict(X_test)\n",
    "      grid_search = RandomizedSearchCV(estimator=model,  param_distributions=param_grids[name], cv=3, scoring='r2',n_iter=4 )\n",
    "      grid_search.fit(X_train, y_train)\n",
    "            \n",
    "      best_model = grid_search.best_estimator_\n",
    "      y_pred = best_model.predict(X_test)\n",
    "            \n",
    "      \n",
    "\n",
    "      mse = mean_squared_error(y_test, y_pred)\n",
    "      mae = mean_absolute_error(y_test, y_pred)\n",
    "      r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "      print(f\"Model: {name}\")\n",
    "      print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "      print(f\"Mean squared error is: {mse}\")\n",
    "      print(f\"Mean absolute error is: {mae}\")\n",
    "      print(f\"R2 score error is: {r2}\")\n",
    "      print(\"\\n\")\n",
    "\n",
    "\n",
    "# now calling the function in order to get the outputs\n",
    "evaluate_model_with_base_parameters(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix =  data.select_dtypes(\"number\").corr()\n",
    "target_features = correlation_matrix[target_variables[0]].drop(target_variables[0])\n",
    "target_features = target_features[abs(target_features)>0.3].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[target_features]\n",
    "y=data[target_variables[0]]\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network=keras.Sequential([\n",
    "    layers.Input(shape=(4,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "'''\n",
    "import keras_tuner as kt\n",
    "tuner = kt.Hyperband(neural_network,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.fit(X_train, y_train, epochs=40, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=neural_network.predict(X_test)\n",
    "\n",
    "# calculating the scores based on the performance of the neural network\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Mean squared error (same as neural_network.evaluate()) is: {mse}\")\n",
    "print(f\"Mean absolute error is: {mae}\")\n",
    "print(f\"R2 score error is: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = neural_network.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
